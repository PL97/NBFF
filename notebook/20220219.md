## :fire: Robust Representation Learning with Self-Distillation for Domain Generalization :fire:
**Date: 2023-02-19** [:link:](https://arxiv.org/abs/2302.06874)
### Key idea 
utilize self-distillation to learn domain-invariant features to improve domain generalization. It is claimed to be the first few work on ViT

#### key design:
![](20230219151050.png)

- feed origional image x to NN to get logit output $p(f(x'))$
- apply data augmentation to image x to get image x'
- calculate all intermiediate feature output and feed them to a fc layer f' to get logit $p(f'(x'))$
- calculate the KL divergence $L(p(f(x))||p(f'(x))) as L_i$
- calculate the KL divergence $L(p(f(x))||p(f'(x'))) as L_a$
- the final loss is a compose of $L = L_ce + \lambda L_i + \gamma L_a$
### Takeway 

- self-distillation can be combined with data augmentation to achieve better robustness
- tools: AutoAugment: [:books:](https://arxiv.org/pdf/1805.09501.pdf) [:hammer:](https://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html)

### Idea 
- Is there any more quantitive way of measure domain generalization and directly optimize it?
- For domain adaption where we have access to the target domain data, how can we achive better performance?
  - a possible solution would be that we can disentangle the domain invariant and sepecific features. For example, we can adopt the two-encoder structure to learn features separately and try to maximize the correlation of invariant features (probbaly more accurate if model the conditional probability) and minimize the correlation of invariant and specific features.


___
___




