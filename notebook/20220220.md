## :fire: Explicit and Implicit Knowledge Distillation via Unlabeled Data :fire:
**Date: 2023-02-20** [:link:](https://arxiv.org/abs/2302.08771) #tag/computer_vision #tag/knowledge-distillation
### Key idea
this paper proposed a knowledge distillation method that consider prediction entropy as a method for selecting subsitute dataset from wild (a vein of knowledge distillation method that make use of unlabeld public dataset). Major components compose of: 1) a adaptive threshold selection. Use entropy as a way to select samples from a dataset. 2) class-dropping noise suppresion. Use confidence to generate pseudo labels (multi-label). 3) feature distillation component. pick two layers in NN, one from the firt BN layer which represent low level features (claimed to boost the convergence speed), the other from the last layer before final linear layer which is used to mimic the output of the teacher model. 4) last is structure differentiation relationships which constrain the difference of the output logits of teacher and student should be close.

:thumbsup: The design of multiple component to suppress either the label noise or imitate the structured differentiation is interesting.

:thumbsdown: There are no much novelty here. More or less like putting pices together to get a dish. 

### Takeway

### Idea

- feature distillation part is too loose. Is there any better way to improve the convergence?
- the final loss term is too many. Which is the most foundimental component lead to such performance boost.
- what if the origional model is not robust, can student model inherite them?
